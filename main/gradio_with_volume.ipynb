{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr # for user interface\n",
    "import json # for retrieving json files from website to get price history\n",
    "import urllib.parse # encoding skin names to so it can be fetched through url \n",
    "import matplotlib.pyplot as plt # visualization \n",
    "import ast  \n",
    "import requests # for web scrapping\n",
    "import seaborn as sns # also for visualization\n",
    "import pandas as pd # data framing processed data from json so that it can be plotted\n",
    "from bs4 import BeautifulSoup # for web scrapping \n",
    "import os \n",
    "import random\n",
    "\n",
    "## All for the LS TM model\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import matplotlib.dates as mdates # date-handling utilities\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.preprocessing import MinMaxScaler # to normalize data within a range\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error # to be used to count MAE and RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load_skins() = Load skins from skins.txt file that was webscrapped from csgoskins.gg. This works from loading the items with format like ('Five-SeveN | Copper Galaxy', True, False) where the first boolean value is the modifier for stattrak and the second for souvenir. We also add the weapon_conditions for each weapons through a nested for loop. \n",
    "- search_list() = Makes a list of tuples to ease the searching process. \n",
    "- load_cases() = Loads cases from cases.txt file that was also webscrapped from csgoskins.gg. The logic is the same as load_skins(|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weapon_conditions = [\n",
    "    \"(Factory New)\", \"(Minimal Wear)\", \"(Field-Tested)\", \"(Well-Worn)\", \"(Battle-Scarred)\"\n",
    "]\n",
    "\n",
    "wear = [\n",
    "    \"Factory New\", \"Minimal Wear\", \"Field-Tested\", \"Well-Worn\", \"Battle-Scarred\"\n",
    "]\n",
    "\n",
    "def load_skins():\n",
    "    with open(\"skins.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        skins = [ast.literal_eval(line.strip()) for line in file.readlines()]  \n",
    "\n",
    "    processed_skins = []\n",
    "    for skin, is_stattrak, is_souvenir in skins:\n",
    "        processed_skins.append(skin)  \n",
    "\n",
    "        if is_stattrak:\n",
    "            processed_skins.append(f\"StatTrak {skin}\")\n",
    "\n",
    "        if is_souvenir:\n",
    "            processed_skins.append(f\"Souvenir {skin}\")\n",
    "\n",
    "    with_wear = []\n",
    "    for skin in processed_skins:\n",
    "        for conditions in weapon_conditions:\n",
    "            with_wear.append(f\"{skin} {conditions}\")\n",
    "\n",
    "    return with_wear\n",
    "\n",
    "def search_list():\n",
    "    with open(\"skins.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        skins = [ast.literal_eval(line.strip()) for line in file.readlines()]  \n",
    "\n",
    "    separated_data = []\n",
    "    for name, val1, val2 in skins:\n",
    "        parts = name.split(\" | \")\n",
    "        \n",
    "        if len(parts) == 2:  \n",
    "            weapon, skin = parts\n",
    "        else:\n",
    "            weapon, skin = name, \"Unknown\"  \n",
    "        weapon = weapon.replace(\"★\", \"\")\n",
    "        separated_data.append((weapon.strip(), skin.strip(), val1, val2))\n",
    "\n",
    "    search_processed_skins = []\n",
    "    for skins in separated_data:\n",
    "        for w in wear:\n",
    "                search_processed_skins.append((skins[0], skins[1], \"Base\", w))\n",
    "\n",
    "        if skins[2]:\n",
    "            for w in wear:\n",
    "                search_processed_skins.append((skins[0], skins[1], \"StatTrak\", w))\n",
    "        elif skins[3]:\n",
    "            for w in wear:\n",
    "                search_processed_skins.append((skins[0], skins[1], \"Souvenir\", w))\n",
    "\n",
    "    return search_processed_skins\n",
    "\n",
    "def load_cases():\n",
    "    with open(\"case.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        cases = [line.strip() for line in file.readlines()]\n",
    "    return cases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Functions and URL Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find_skin(sr_format, url_format, wep, s, w, m)\n",
    "    - sr_format: result value from the search_list()\n",
    "    - url_format: result value from the load_skin()\n",
    "    - wep: name of the weapon \n",
    "    - s: name of the skin \n",
    "    - w: wear of the skin \n",
    "    - m: modifier of the skin(stattrak/souvenir)\n",
    "- find_skin(): Works by matching the values given through the parameters with the list of tuples made through search_list(). Matched through their indexes. \n",
    "- find_case(c_data, name): Applies a simpler logic where it takes the data loaded from load_cases() function which is used in the c_data parameter. The name is also given to the function to match it with existing data. \n",
    "- create_url(item): Creates the link to the site containing the price history of an item in the steam market. The json will be downloaded in another phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_skin(sr_format, url_format, wep,s,w,m):\n",
    "    input_weapon = wep.lower()\n",
    "    input_skin = s.lower() \n",
    "    input_wear = w.lower()\n",
    "    input_modifier = m.lower()\n",
    "    i = 0\n",
    "    for items in sr_format:\n",
    "        weapon = items[0].lower()\n",
    "        skin = items[1].lower()\n",
    "        modifier = items[2].lower()\n",
    "        wear = items[3].lower()\n",
    "        #print(skin,modifier,wear)\n",
    "        if skin == input_skin and modifier == input_modifier and wear == input_wear and weapon == input_weapon:               \n",
    "            print(f\"Item Found in index : {i}\")\n",
    "            print(weapon, modifier, skin, wear)\n",
    "            return url_format[i]\n",
    "        i = i + 1\n",
    "    return None \n",
    "\n",
    "def find_case(c_data, name):\n",
    "    for cases in c_data:\n",
    "        if cases.lower() == name.lower():\n",
    "            return cases \n",
    "\n",
    "def create_url(item):\n",
    "    url_format_base = \"http://steamcommunity.com/market/pricehistory/?appid=730&market_hash_name=\"\n",
    "    formatted = url_format_base+urllib.parse.quote(item, safe=\"★\")\n",
    "    return formatted.replace(\"%E2%98%85\", \"★\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get JSON\n",
    "- get_json(url, file_name): just like its name says, it takes in the url made from create_url and gets the json from that site. It will be saved as the provided name through file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this also has the necessary cookies required to fetch the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For this step, it is important to have a steam account because without a steam account, you cannot fetch the price history of the item. The cookies are the ones that allows us to fetch the price history data of an item. These cookies can change once in a while so it is possible that the cookies written right now does not work. To get the cookies, you need to inspect the page where steam is logged in at and go to applications and find the sessionid and steamLoginSecure values to ensure that this part can run. Use the temporary account and replace cookie values with the logged in account. Inspect it on https://steamcommunity.com/market/ and make sure you are logged in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporary Steam Account: \n",
    "- Username: dia_project_2025\n",
    "- Pass: Project2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = { \n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Referer\": \"https://steamcommunity.com\",\n",
    "}\n",
    "# if it doesn't work, try another cookie \n",
    "cookies = { # make sure to censor in github\n",
    "    \"sessionid\": \"8aef10bfacdf994fdcf987c4\",\n",
    "    \"steamLoginSecure\": \"76561199837440278%7C%7CeyAidHlwIjogIkpXVCIsICJhbGciOiAiRWREU0EiIH0.eyAiaXNzIjogInI6MDAwNl8yNjBDRDBERl9FMDcwNCIsICJzdWIiOiAiNzY1NjExOTk4Mzc0NDAyNzgiLCAiYXVkIjogWyAid2ViOmNvbW11bml0eSIgXSwgImV4cCI6IDE3NDMyNjU0MjksICJuYmYiOiAxNzM0NTM3MzkwLCAiaWF0IjogMTc0MzE3NzM5MCwgImp0aSI6ICIwMDAxXzI2MENEMEVGX0JBOUE1IiwgIm9hdCI6IDE3NDMwODk3NTgsICJydF9leHAiOiAxNzYxMzg2NzgyLCAicGVyIjogMCwgImlwX3N1YmplY3QiOiAiMjAzLjIxNy4xMzEuMTcwIiwgImlwX2NvbmZpcm1lciI6ICIyMDMuMjE3LjEzMS4xNzAiIH0.JG4dJBMKifLXh1Ciy1ZXGNLc5pIrVIanqjMNWuC-DSoYav3KITO8rXURa33FII9TYlI9gMl80aqLSXXL2cXvDQ\",\n",
    "}\n",
    "\n",
    "def get_json(url, file_name):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, cookies=cookies)\n",
    "        if response.status_code == 400:\n",
    "            print(f\"400 Bad Request for {url}: {response.text}\") \n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        filename = f\"{file_name}.json\"  \n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "        print(f\"Saved JSON as {filename} in the root directory.\")\n",
    "        return True  \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Image\n",
    "- This function is quite simple, you put in the name of the file and go to the listing page and then scrape the image from the site, download it to the same folder and then display it with gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_item_image(item, filename=\"item.jpg\"):\n",
    "    base_link = \"https://steamcommunity.com/market/listings/730/\"\n",
    "    formatted = base_link + urllib.parse.quote(item, safe=\"★\").replace(\"%E2%98%85\", \"★\")\n",
    "\n",
    "    print(formatted)\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(formatted, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    image_div = soup.find(\"div\", class_=\"market_listing_largeimage\")\n",
    "    if not image_div:\n",
    "        return \"Div not found.\"\n",
    "\n",
    "    img_tag = image_div.find(\"img\", {\"src\": True})\n",
    "    if not img_tag:\n",
    "        return \"Image not found.\"\n",
    "\n",
    "    image_url = img_tag[\"src\"]\n",
    "    img_response = requests.get(image_url, headers=headers)\n",
    "\n",
    "    if img_response.status_code == 200:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        \n",
    "        file_path = os.path.join(\"images\", filename)\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(img_response.content)\n",
    "        \n",
    "        return file_path  \n",
    "    else:\n",
    "        return \"Failed to download image.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Function to be used for Prediction\n",
    "- preprocess_data(file_path): Remove duplicates dates, taking the average price, and set proper date index.\n",
    "- create_sequences(): Prepares time series data for machine learning models, particularly for sequential models like LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path):\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file) # Load the file\n",
    "    \n",
    "    df = pd.DataFrame(data['prices'], columns=['Date', 'Price', 'Volume']) # Turns the json file into a dataframe\n",
    "    df['Price'] = df['Price'].astype(float) # Make sure that price is a float type\n",
    "    df['Volume'] = df['Volume'].astype(int) # Make sure that volume is a int type\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%b %d %Y %H: +0', errors='coerce') # Make sures that the date is formated correctly\n",
    "    \n",
    "    # Group by date, averaging Price & summing Volume\n",
    "    df_grouped = df.groupby(df['Date'].dt.date).agg({'Price': 'mean', 'Volume': 'sum'}).reset_index()\n",
    "    df_grouped.rename(columns={'Date': 'Date', 'Price': 'Price', 'Volume': 'Volume'}, inplace=True)\n",
    "    \n",
    "    # Convert Date column to datetime and set as index\n",
    "    df_grouped['Date'] = pd.to_datetime(df_grouped['Date'])\n",
    "    df_grouped.set_index('Date', inplace=True)\n",
    "    \n",
    "    return df_grouped\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])  # Input features (Price & Volume)\n",
    "        y.append(data[i+seq_length][0])  # Predicting only Price\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Prediction | ARIMA\n",
    "- Predict prices using ARIMA model and plot actual vs predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price_arima(name):\n",
    "    try:\n",
    "        # To keep track of the time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        df = preprocess_data('item.json')\n",
    "\n",
    "        # Ensure the datetime index has a proper frequency\n",
    "        df = df.asfreq('D').interpolate()\n",
    "\n",
    "        # Scale both Price and Volume\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        df[['Scaled_Price', 'Scaled_Volume']] = scaler.fit_transform(df[['Price', 'Volume']])\n",
    "\n",
    "        # Split into training and test sets\n",
    "        train_size = len(df) - 30  # Use all but last 30 days for training\n",
    "        train_price, test_price = df['Scaled_Price'].iloc[:train_size], df['Scaled_Price'].iloc[train_size:]\n",
    "        train_volume, test_volume = df['Scaled_Volume'].iloc[:train_size], df['Scaled_Volume'].iloc[train_size:]\n",
    "\n",
    "        # Fit ARIMA model with Volume as exogenous variable\n",
    "        model = ARIMA(train_price, order=(5,1,0), exog=train_volume)\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Forecast the next 30 days with exogenous volume data\n",
    "        predictions = model_fit.forecast(steps=30, exog=test_volume)\n",
    "\n",
    "        # Reverse scaling for price predictions\n",
    "        predictions = scaler.inverse_transform(np.column_stack((predictions, np.zeros_like(predictions))))[:, 0]\n",
    "\n",
    "        # Get actual prices\n",
    "        actual_prices = df['Price'].iloc[train_size:].values\n",
    "        future_dates = df.index[train_size:]\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        mae = mean_absolute_error(actual_prices, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(actual_prices, predictions))\n",
    "        mape = np.mean(np.abs((actual_prices - predictions) / actual_prices)) * 100\n",
    "\n",
    "        print(f\"MAE ARIMA: {mae:.4f}\")\n",
    "        print(f\"MAPE ARIMA: {mape:.4f}%\")\n",
    "        print(f\"RMSE ARIMA: {rmse:.4f}\")\n",
    "\n",
    "        # # Calculate price difference\n",
    "        # last_actual_price = df['Price'].iloc[train_size - 1]  # Last actual price before prediction\n",
    "        # last_predicted_price = predictions[-1, 0]  # Last predicted price after 30 days\n",
    "        # price_change = last_predicted_price - last_actual_price\n",
    "        # percentage_change = (price_change / last_actual_price) * 100\n",
    "\n",
    "        # print(f\"Price Change: {price_change:.2f}\")\n",
    "        # print(f\"Percentage Change: {percentage_change:.2f}%\")\n",
    "\n",
    "        # Plot actual vs predicted prices\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df.index[-60:], df['Price'][-60:], label='Actual Prices', marker='o')\n",
    "        plt.plot(future_dates, predictions, label='Predicted Prices (ARIMA)', linestyle='dashed', color='red', marker='o')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.title(f'ARIMA Backtesting (Actual vs Predicted - 30 Days) for {name}')\n",
    "\n",
    "        # Save the plot\n",
    "        predict_path_arima = \"images/prediction_arima.png\"\n",
    "        plt.savefig(predict_path_arima)\n",
    "        plt.close()\n",
    "\n",
    "        # Calculate execution time\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution Time: {execution_time:.4f} seconds \\n\")\n",
    "\n",
    "        return predict_path_arima\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Prediction | LSTM\n",
    "- Predict prices using LSTM model and plot actual vs predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price_lstm(name):\n",
    "    try:\n",
    "        # Clear previous TensorFlow sessions\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "        # To keep track of execution time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        df = preprocess_data('item.json')\n",
    "\n",
    "        # Ensure the datetime index has a proper frequency\n",
    "        df = df.asfreq('D').interpolate()\n",
    "\n",
    "        # Scale both Price and Volume\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        df[['Scaled_Price', 'Scaled_Volume']] = scaler.fit_transform(df[['Price', 'Volume']])\n",
    "\n",
    "        # Split into training and test sets\n",
    "        seq_length = 30\n",
    "        train_size = len(df) - seq_length\n",
    "        train_data = df[['Scaled_Price', 'Scaled_Volume']].iloc[:train_size].values\n",
    "        test_data = df[['Scaled_Price', 'Scaled_Volume']].iloc[train_size:].values\n",
    "\n",
    "        X_train, y_train = create_sequences(train_data, seq_length)\n",
    "        X_test, y_test = create_sequences(np.concatenate([train_data[-seq_length:], test_data]), seq_length)\n",
    "\n",
    "        # Reshape input for LSTM (Samples, Time Steps, Features)\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 2))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 2))\n",
    "\n",
    "        # Define LSTM model\n",
    "        model = Sequential([\n",
    "            Input(shape=(seq_length, 2)),  \n",
    "            LSTM(50, return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(25, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        # Use EarlyStopping to prevent overfitting\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0, callbacks=[early_stopping]) \n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions = scaler.inverse_transform(np.column_stack((predictions, np.zeros_like(predictions))))[:, 0]\n",
    "\n",
    "        # Get actual prices\n",
    "        actual_prices = df['Price'].iloc[train_size:].values\n",
    "        future_dates = df.index[train_size:]\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        mae = mean_absolute_error(actual_prices, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(actual_prices, predictions))\n",
    "        mape = np.mean(np.abs((actual_prices - predictions) / actual_prices)) * 100\n",
    "\n",
    "        print(f\"MAE LSTM: {mae:.4f}\")\n",
    "        print(f\"MAPE LSTM: {mape:.4f}%\")\n",
    "        print(f\"RMSE LSTM: {rmse:.4f}\")\n",
    "\n",
    "        # Plot actual vs predicted prices\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df.index[-60:], df['Price'][-60:], label='Actual Prices', marker='o')\n",
    "        plt.plot(future_dates, predictions, label='Predicted Prices (LSTM)', linestyle='dashed', color='red', marker='o')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.title(f'LSTM Backtesting (Actual vs Predicted - 30 Days) for {name}')\n",
    "\n",
    "        # Save the plot\n",
    "        predict_path = \"images/prediction_lstm.png\"\n",
    "        plt.savefig(predict_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Calculate execution time\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution Time: {execution_time:.4f} seconds \\n\")\n",
    "\n",
    "        return predict_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Prediction | XGBoost\n",
    "- Predict prices using XGBoost model and plot actual vs predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price_xgboost(name):\n",
    "    try:\n",
    "        # To keep track of the time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        df = preprocess_data('item.json')\n",
    "\n",
    "        # Ensure the datetime index has a proper frequency\n",
    "        df = df.asfreq('D').interpolate()\n",
    "\n",
    "        # Scale both Price and Volume\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        df[['Scaled_Price', 'Scaled_Volume']] = scaler.fit_transform(df[['Price', 'Volume']])\n",
    "\n",
    "        # Split into training and test sets\n",
    "        seq_length = 30\n",
    "        train_size = len(df) - seq_length \n",
    "        train_data = df[['Scaled_Price', 'Scaled_Volume']].iloc[:train_size].values\n",
    "        test_data = df[['Scaled_Price', 'Scaled_Volume']].iloc[train_size:].values\n",
    "\n",
    "        # Prepare sequences\n",
    "        X_train, y_train = create_sequences(train_data, seq_length)\n",
    "        X_test, y_test = create_sequences(np.concatenate([train_data[-seq_length:], test_data]), seq_length)\n",
    "\n",
    "        # Flatten input for XGBoost\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)  # 2D shape for XGBoost\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "        # Define XGBoost model\n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.5, max_depth=5)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Reverse scaling for price predictions\n",
    "        predictions = scaler.inverse_transform(np.column_stack((predictions, np.zeros_like(predictions))))[:, 0]\n",
    "\n",
    "        # Get actual prices\n",
    "        actual_prices = df['Price'].iloc[train_size:].values\n",
    "        future_dates = df.index[train_size:]\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        mae = mean_absolute_error(actual_prices, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(actual_prices, predictions))\n",
    "        mape = np.mean(np.abs((actual_prices - predictions) / actual_prices)) * 100\n",
    "\n",
    "        print(f\"MAE XGBoost: {mae:.4f}\")\n",
    "        print(f\"MAPE XGBoost: {mape:.4f}%\")\n",
    "        print(f\"RMSE XGBoost: {rmse:.4f}\")\n",
    "\n",
    "        # # Calculate price difference\n",
    "        # last_actual_price = df['Price'].iloc[train_size - 1]  # Last actual price before prediction\n",
    "        # last_predicted_price = predictions[-1, 0]  # Last predicted price after 30 days\n",
    "        # price_change = last_predicted_price - last_actual_price\n",
    "        # percentage_change = (price_change / last_actual_price) * 100\n",
    "\n",
    "        # print(f\"Price Change: {price_change:.2f}\")\n",
    "        # print(f\"Percentage Change: {percentage_change:.2f}%\")\n",
    "\n",
    "        # Plot actual vs predicted prices\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df.index[-60:], df['Price'][-60:], label='Actual Prices', marker='o')\n",
    "        plt.plot(future_dates, predictions, label='Predicted Prices (XGBoost)', linestyle='dashed', color='red', marker='o')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.title(f'XGBoost Backtesting (Actual vs Predicted - 30 Days) for {name}')\n",
    "\n",
    "        # Save the plot\n",
    "        predict_path = \"images/prediction_xgboost.png\"\n",
    "        plt.savefig(predict_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Calculate execution time\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution Time: {execution_time:.4f} seconds \\n\")\n",
    "\n",
    "        return predict_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the functions and how they work are implemented into two functions which are:\n",
    "- process_skin_input(): Takes in user inputs and process them \n",
    "- process_case_input(): Does the same thing, just for cases \n",
    "\n",
    "This then is used in conjunction with the gradio interface to make it easier for users to interact with the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://steamcommunity.com/market/listings/730/Shadow%20Case\n",
      "Saved JSON as item.json in the root directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vagan\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ARIMA: 0.0938\n",
      "MAPE ARIMA: 4.5221%\n",
      "RMSE ARIMA: 0.1195\n",
      "Execution Time: 13.6724 seconds \n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\vagan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617ms/step\n",
      "MAE LSTM: 0.1799\n",
      "MAPE LSTM: 8.9260%\n",
      "RMSE LSTM: 0.1860\n",
      "Execution Time: 169.9281 seconds \n",
      "\n",
      "MAE XGBoost: 0.2134\n",
      "MAPE XGBoost: 10.3562%\n",
      "RMSE XGBoost: 0.2421\n",
      "Execution Time: 1.0973 seconds \n",
      "\n",
      "https://steamcommunity.com/market/listings/730/Shattered%20Web%20Case\n",
      "Saved JSON as item.json in the root directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vagan\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ARIMA: 0.3019\n",
      "MAPE ARIMA: 4.9617%\n",
      "RMSE ARIMA: 0.3569\n",
      "Execution Time: 6.0442 seconds \n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step\n",
      "MAE LSTM: 0.1208\n",
      "MAPE LSTM: 2.0177%\n",
      "RMSE LSTM: 0.1425\n",
      "Execution Time: 133.0937 seconds \n",
      "\n",
      "MAE XGBoost: 0.6196\n",
      "MAPE XGBoost: 10.2845%\n",
      "RMSE XGBoost: 0.6696\n",
      "Execution Time: 0.7013 seconds \n",
      "\n",
      "https://steamcommunity.com/market/listings/730/Snakebite%20Case\n",
      "Saved JSON as item.json in the root directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vagan\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ARIMA: 0.0444\n",
      "MAPE ARIMA: 11.5069%\n",
      "RMSE ARIMA: 0.0500\n",
      "Execution Time: 6.9045 seconds \n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
      "MAE LSTM: 0.0110\n",
      "MAPE LSTM: 2.7999%\n",
      "RMSE LSTM: 0.0131\n",
      "Execution Time: 72.4893 seconds \n",
      "\n",
      "MAE XGBoost: 0.0083\n",
      "MAPE XGBoost: 2.0631%\n",
      "RMSE XGBoost: 0.0106\n",
      "Execution Time: 0.3726 seconds \n",
      "\n",
      "https://steamcommunity.com/market/listings/730/Spectrum%202%20Case\n",
      "Saved JSON as item.json in the root directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vagan\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ARIMA: 0.1004\n",
      "MAPE ARIMA: 2.8441%\n",
      "RMSE ARIMA: 0.1187\n",
      "Execution Time: 8.4104 seconds \n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
      "MAE LSTM: 0.4466\n",
      "MAPE LSTM: 12.4757%\n",
      "RMSE LSTM: 0.4503\n",
      "Execution Time: 154.8743 seconds \n",
      "\n",
      "MAE XGBoost: 0.1056\n",
      "MAPE XGBoost: 2.9870%\n",
      "RMSE XGBoost: 0.1254\n",
      "Execution Time: 0.4088 seconds \n",
      "\n",
      "https://steamcommunity.com/market/listings/730/Spectrum%20Case\n",
      "Saved JSON as item.json in the root directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vagan\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ARIMA: 0.1482\n",
      "MAPE ARIMA: 2.7939%\n",
      "RMSE ARIMA: 0.1827\n",
      "Execution Time: 7.4094 seconds \n",
      "\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000204D63E59E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step\n",
      "MAE LSTM: 0.5844\n",
      "MAPE LSTM: 11.1603%\n",
      "RMSE LSTM: 0.5904\n",
      "Execution Time: 168.1339 seconds \n",
      "\n",
      "MAE XGBoost: 0.1109\n",
      "MAPE XGBoost: 2.1106%\n",
      "RMSE XGBoost: 0.1315\n",
      "Execution Time: 0.6658 seconds \n",
      "\n",
      "https://steamcommunity.com/market/listings/730/Winter%20Offensive%20Weapon%20Case\n",
      "Saved JSON as item.json in the root directory.\n",
      "MAE ARIMA: 0.3726\n",
      "MAPE ARIMA: 3.9831%\n",
      "RMSE ARIMA: 0.4386\n",
      "Execution Time: 4.6583 seconds \n",
      "\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000204CDD62FC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step\n",
      "MAE LSTM: 0.4212\n",
      "MAPE LSTM: 4.5199%\n",
      "RMSE LSTM: 0.4875\n",
      "Execution Time: 244.8098 seconds \n",
      "\n",
      "MAE XGBoost: 0.2154\n",
      "MAPE XGBoost: 2.3294%\n",
      "RMSE XGBoost: 0.2736\n",
      "Execution Time: 0.7970 seconds \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_skin_input(weapon_input, skin_name, condition, modifiers):\n",
    "    url_format = load_skins()  # Load skin list\n",
    "    sr_format = search_list()  # Load search list\n",
    "\n",
    "    search = find_skin(sr_format, url_format, weapon_input, skin_name, condition, modifiers)\n",
    "    \n",
    "    if not search:\n",
    "        return \"Skin not found. Please check your input.\", None, None, None\n",
    "    \n",
    "    img = fetch_item_image(search)\n",
    "\n",
    "    url = create_url(search)\n",
    "    get_json(url, \"item\")  \n",
    "\n",
    "    file_path = \"item.json\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.DataFrame(data[\"prices\"], columns=[\"timestamp\", \"price\", \"volume\"])\n",
    "    df[\"price\"] = df[\"price\"].astype(float)\n",
    "    df[\"volume\"] = df[\"volume\"].astype(int)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%b %d %Y %H: +0\", errors=\"coerce\")\n",
    "    df = df.dropna().sort_values(\"timestamp\")\n",
    "\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x=df[\"timestamp\"], y=df[\"price\"], color=\"b\", label=\"Price\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(f\"Price Over Time for {search}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    temp_path = \"images/temp_chart.png\"\n",
    "    plt.savefig(temp_path)\n",
    "    plt.close()\n",
    "\n",
    "    return search, img, temp_path, predict_price_arima(search), predict_price_lstm(search), predict_price_xgboost(search)\n",
    "\n",
    "def process_case_input(case_name):\n",
    "    case_data = load_cases()\n",
    "    search = find_case(case_data, case_name)\n",
    "\n",
    "    if not search:\n",
    "        return \"Case not found. Please check your input.\", None, None, None\n",
    "\n",
    "    img = fetch_item_image(search)\n",
    "\n",
    "    url = create_url(search)\n",
    "    get_json(url, \"item\")  \n",
    "\n",
    "    file_path = \"item.json\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.DataFrame(data[\"prices\"], columns=[\"timestamp\", \"price\", \"volume\"])\n",
    "    df[\"price\"] = df[\"price\"].astype(float)\n",
    "    df[\"volume\"] = df[\"volume\"].astype(int)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%b %d %Y %H: +0\", errors=\"coerce\")\n",
    "    df = df.dropna().sort_values(\"timestamp\")\n",
    "\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x=df[\"timestamp\"], y=df[\"price\"], color=\"b\", label=\"Price\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(f\"Price Over Time for {search}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    temp_path = \"images/temp_chart.png\"\n",
    "    plt.savefig(temp_path)\n",
    "    plt.close()\n",
    "\n",
    "    return search, img, temp_path, predict_price_arima(search), predict_price_lstm(search), predict_price_xgboost(search)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Counter-Strike Market Analyzer\")\n",
    "    selection = gr.Radio([\"Skins\", \"Case\", \"Knifes and Gloves\"], label=\"Select Item Type [Skins/Case]\", value=None)\n",
    "\n",
    "    with gr.Column(visible=False) as skins_interface: \n",
    "        with gr.Row():\n",
    "            weapon_input = gr.Dropdown(\n",
    "                    ['AK-47', 'AUG', 'AWP', 'CZ75-Auto', 'Desert Eagle', 'Dual Berettas', 'FAMAS', 'Five-SeveN', \n",
    "                    'G3SG1', 'Galil AR', 'Glock-18', 'M249', 'M4A1-S', 'M4A4', 'MAG-7', 'MAC-10', 'MP5-SD', 'MP9', \n",
    "                    'Negev', 'Nova', 'P2000', 'P250', 'P90', 'PP-Bizon', 'R8 Revolver', 'SCAR-20', 'SG 553', \n",
    "                    'SSG 08', 'Sawed-Off', 'Tec-9', 'UMP-45', 'USP-S', 'XM1014', 'Zeus x27'],\n",
    "                    label=\"Select a Weapon\"\n",
    "                )\n",
    "            skin_name = gr.Textbox(label=\"Enter Skin Name: \", placeholder=\"e.g, Dragon Lore\")\n",
    "        with gr.Row():\n",
    "            condition = gr.Dropdown([\"Factory New\", \"Minimal Wear\", \"Field Tested\", \"Well Worn\", \"Battle-Scarred\"], label=\"Condition\")\n",
    "            modifiers = gr.Dropdown([ \"Base\", \"StatTrak\", \"Souvenir\"], label=\"Pick the Modifiers\")\n",
    "        btn1 = gr.Button(\"Submit\")\n",
    "       \n",
    "        text_msg = gr.Textbox(label=\"Result\", interactive=False)\n",
    "        item_image = gr.Image(label=\"Item Image\")\n",
    "        with gr.Row():\n",
    "            priceChart = gr.Image(label=\"Price Trend\")\n",
    "            predictChartARIMA = gr.Image(label=\"Price Prediction ARIMA\")\n",
    "            predictChartLSTM = gr.Image(label=\"Price Prediction LSTM\")\n",
    "            predictChartXGB = gr.Image(label=\"Price Prediction XGB\")\n",
    "\n",
    "\n",
    "        btn1.click(\n",
    "            fn=process_skin_input,\n",
    "            inputs=[weapon_input, skin_name, condition, modifiers],\n",
    "            outputs=[text_msg, item_image, priceChart, predictChartARIMA, predictChartLSTM, predictChartXGB]\n",
    "        )\n",
    "\n",
    "    with gr.Column(visible=False) as cases_interface:  \n",
    "        case_input = gr.Dropdown(\n",
    "                    [\n",
    "                    \"Gallery Case\", \"Kilowatt Case\", \"Revolution Case\", \"Recoil Case\", \"Dreams & Nightmares Case\",\n",
    "                    \"Chroma 2 Case\", \"Chroma 3 Case\", \"Chroma Case\", \"Clutch Case\", \"CS20 Case\", \"CS:GO Weapon Case\", \n",
    "                    \"CS:GO Weapon Case 2\", \"CS:GO Weapon Case 3\", \"Danger Zone Case\", \"eSports 2013 Case\", \n",
    "                    \"eSports 2013 Winter Case\", \"eSports 2014 Summer Case\", \"Falchion Case\", \"Fracture Case\", \"Gamma 2 Case\", \n",
    "                    \"Gamma Case\", \"Glove Case\", \"Horizon Case\", \"Huntsman Weapon Case\", \"Operation Bravo Case\", \"Operation Breakout Weapon Case\",\n",
    "                    \"Operation Broken Fang Case\", \"Operation Hydra Case\", \"Operation Phoenix Weapon Case\", \"Operation Riptide Case\",\n",
    "                    \"Operation Vanguard Weapon Case\", \"Operation Wildfire Case\", \"Prisma 2 Case\", \"Prisma Case\", \"Revolver Case\",\n",
    "                    \"Shadow Case\", \"Shattered Web Case\", \"Snakebite Case\", \"Spectrum 2 Case\", \"Spectrum Case\", \"Winter Offensive Weapon Case\"\n",
    "                    ],\n",
    "                    label=\"Select a Case\"\n",
    "                )\n",
    "        btn2 = gr.Button(\"Submit\")\n",
    "       \n",
    "        text_msg = gr.Textbox(label=\"Result\", interactive=False)\n",
    "        item_image = gr.Image(label=\"Item Image\")\n",
    "        with gr.Row():\n",
    "            priceChart = gr.Image(label=\"Price Trend\")\n",
    "            predictChartARIMA = gr.Image(label=\"Price Prediction ARIMA\")\n",
    "            predictChartLSTM = gr.Image(label=\"Price Prediction LSTM\")\n",
    "            predictChartXGB = gr.Image(label=\"Price Prediction XGB\")\n",
    "\n",
    "        btn2.click(\n",
    "            fn=process_case_input,\n",
    "            inputs=[case_input],\n",
    "            outputs=[text_msg, item_image, priceChart, predictChartARIMA, predictChartLSTM, predictChartXGB]\n",
    "        )\n",
    "\n",
    "    with gr.Column(visible=False) as kg_interface:  \n",
    "        with gr.Row():\n",
    "            kg_input = gr.Dropdown(\n",
    "                    [\n",
    "                    \"Bayonet\", \"Bowie Knife\", \"Butterfly Knife\", \"Classic Knife\",\n",
    "                    \"Falchion Knife\", \"Flip Knife\", \"Gut Knife\", \"Huntsman Knife\",\n",
    "                    \"Karambit\", \"Kukri Knife\", \"M9 Bayonet\", \"Navaja Knife\", \"Nomad Knife\",\n",
    "                    \"Paracord Knife\", \"Shadow Daggers\", \"Skeleton Knife\", \"Stiletto Knife\",\n",
    "                    \"Survival Knife\", \"Talon Knife\", \"Ursus Knife\",\n",
    "                    \"Bloodhound Gloves\", \"Broken Fang Gloves\", \"Driver Gloves\", \"Hand Wraps\",\n",
    "                    \"Hydra Gloves\", \"Moto Gloves\", \"Specialist Gloves\", \"Sport Gloves\"\n",
    "                    ],\n",
    "                    label=\"Select a Weapon\"\n",
    "                )\n",
    "            skin_name = gr.Textbox(label=\"Enter Skin Name: \", placeholder=\"e.g, Dragon Lore\")\n",
    "        with gr.Row():\n",
    "            condition = gr.Dropdown([\"Factory New\", \"Minimal Wear\", \"Field Tested\", \"Well Worn\", \"Battle-Scarred\"], label=\"Condition\")\n",
    "            modifiers = gr.Dropdown([\"Base\",\"StatTrak\"], label=\"Pick the Modifiers\")\n",
    "        btn3 = gr.Button(\"Submit\")\n",
    "       \n",
    "        text_msg = gr.Textbox(label=\"Result\", interactive=False)\n",
    "        item_image = gr.Image(label=\"Item Image\")\n",
    "        with gr.Row():\n",
    "            priceChart = gr.Image(label=\"Price Trend\")\n",
    "            predictChartARIMA = gr.Image(label=\"Price Prediction ARIMA\")\n",
    "            predictChartLSTM = gr.Image(label=\"Price Prediction LSTM\")\n",
    "            predictChartXGB = gr.Image(label=\"Price Prediction XGB\")\n",
    "\n",
    "        btn3.click(\n",
    "            fn=process_skin_input,\n",
    "            inputs=[kg_input, skin_name, condition, modifiers],\n",
    "            outputs=[text_msg, item_image, predictChartARIMA, predictChartLSTM, predictChartXGB]\n",
    "        )\n",
    "\n",
    "\n",
    "    def select_type(selection):\n",
    "        if selection == \"Skins\":\n",
    "            return gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
    "        elif selection == \"Case\":\n",
    "            return gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)\n",
    "        elif selection == \"Knifes and Gloves\":\n",
    "            return gr.update(visible=False), gr.update(visible=False), gr.update(visible=True)\n",
    "        else:\n",
    "            return gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n",
    "\n",
    "\n",
    "    selection.change(\n",
    "        fn=select_type,\n",
    "        inputs=[selection],\n",
    "        outputs=[skins_interface, cases_interface, kg_interface]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
